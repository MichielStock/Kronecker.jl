@article{Besard2019,
abstract = {GPUs and other accelerators are popular devices for accelerating compute-intensive, parallelizable applications. However, programming these devices is a difficult task. Writing efficient device code is challenging, and is typically done in a low-level programming language. High-level languages are rarely supported, or do not integrate with the rest of the high-level language ecosystem. To overcome this, we propose compiler infrastructure to efficiently add support for new hardware or environments to an existing programming language. We evaluate our approach by adding support for NVIDIA GPUs to the Julia programming language. By integrating with the existing compiler, we significantly lower the cost to implement and maintain the new compiler, and facilitate reuse of existing application code. Moreover, use of the high-level Julia programming language enables new and dynamic approaches for GPU programming. This greatly improves programmer productivity, while maintaining application performance similar to that of the official NVIDIA CUDA toolkit.},
archivePrefix = {arXiv},
arxivId = {1712.03112v1},
author = {Besard, Tim and Foket, Christophe and {De Sutter}, Bjorn},
doi = {10.1109/TPDS.2018.2872064},
eprint = {1712.03112v1},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Besard, Foket, Sutter - Unknown - IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS 1 Effective Extensible Programming Unleashing Ju.pdf:pdf},
issn = {15582183},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Graphics processors,code generation,retargetable compilers,very high-level languages},
number = {4},
pages = {827--841},
title = {{Effective extensible programming: unleashing Julia on GPUs}},
url = {https://arxiv.org/pdf/1712.03112.pdf},
volume = {30},
year = {2019}
}
@article{Innes2019,
abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe Zygote, a Differentiable Programming system that is able to take gradients of general program structures. We implement this system in the Julia programming language. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning, but more importantly, it enables us to incorporate a large ecosystem of libraries in our models in a straightforward way. We discuss our approach to automatic differentiation, including its support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present several examples of differentiating programs.},
archivePrefix = {arXiv},
arxivId = {1907.07587},
author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B and Tebbutt, Will},
eprint = {1907.07587},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Innes et al. - Unknown - ∂P A Differentiable Programming System to Bridge Machine Learning and Scientific Computing.pdf:pdf},
title = {{A differentiable programming system to bridge machine learning and scientific computing}},
url = {https://arxiv.org/pdf/1907.07587.pdf http://arxiv.org/abs/1907.07587},
year = {2019}
}
@techreport{Sch2013,
abstract = {In this paper, we review basic properties of the Kronecker product, and give an overview of its history and applications. We then move on to introducing the symmetric Kronecker product, and we derive sev- eral of its properties. Furthermore, we show its application in finding search directions in semidefinite programming.},
author = {Sch{\"{a}}cke, Kathrin},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{a}}cke - 2013 - On the Kronecker Product.pdf:pdf},
pages = {1--35},
title = {{On the Kronecker Product}},
url = {https://www.math.uwaterloo.ca/{~}hwolkowi/henry/reports/kronthesisschaecke04.pdf},
year = {2013}
}
@article{Stock2017tskrr,
author = {Stock, Michiel and Pahikkala, Tapio and Airola, Antti and {De Baets}, Bernard and Waegeman, Willem},
doi = {10.1162/neco_a_01096},
journal = {Neural Computation},
number = {8},
pages = {2245--2283},
title = {{A comparative study of pairwise learning methods based on kernel ridge regression}},
volume = {30},
year = {2018}
}
@article{Airola2017genvectric,
abstract = {Kronecker product kernel provides the standard approach in the kernel methods literature for learning from pair-input data, where both data points and prediction tasks have their own feature representations. The methods allow simultaneous generalization to both new tasks and data unobserved in the training sample, a setting known as zero-shot or zero-data learning. Such a setting occurs in numerous applications, including drug-target interaction prediction, collaborative filtering and information retrieval. Efficient training algorithms based on the so-called vec trick, that makes use of the special structure of the Kronecker product, are known for the case where the output matrix for the training sample is fully observed, i.e. the correct output for each data point-task combination is available. In this work we generalize these results, proposing an efficient algorithm for sampled Kronecker product multiplication, where only a subset of the full Kronecker product, that corresponds to the training sample, is computed. This allows us to derive a general framework for training Kronecker kernel methods, as specific examples we implement Kronecker ridge regression and support vector machine algorithms. Experimental results demonstrate that the proposed approach leads to accurate models, while allowing order of magnitude improvements in training and prediction time.},
archivePrefix = {arXiv},
arxivId = {1601.01507},
author = {Airola, Antti and Pahikkala, Tapio},
doi = {10.1109/TNNLS.2017.2727545},
eprint = {1601.01507},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Airola, Pahikkala - 2017 - Fast Kronecker product kernel methods via sampled vec trick.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Index Terms kernel methods,Kronecker product kernel,bipartite graph learning,kronecker,linear algebra,ridge regression,support vector machine,transfer learning,zero-shot learning},
mendeley-tags = {kronecker,linear algebra},
number = {8},
pages = {3374--3387},
title = {{Fast Kronecker product kernel methods via generalized vec trick}},
url = {https://arxiv.org/pdf/1601.01507.pdf http://arxiv.org/abs/1601.01507},
volume = {29},
year = {2018}
}
@article{VanLoan2000,
abstract = {The Kronecker product has a rich and very pleasing algebra that supports a wide range of fast, elegant, and practical algorithms. Several trends in scientific computing suggest that this important matrix operation will have an increasingly greater role to play in the future. First, the application areas where Kronecker products abound are all thriving. These include signal processing, image processing, semidefinite programming, and quantum computing. Second, sparse factorizations and Kronecker products are proving to be a very effective way to look at fast linear transforms. Researchers have taken the Kronecker methodology as developed for the fast Fourier transform and used it to build exciting alternatives. Third, as computers get more powerful, researchers are more willing to entertain problems of high dimension and this leads to Kronecker products whenever low-dimension techniques are “tensored” together.},
author = {{Van Loan}, Charles F.},
doi = {10.1016/S0377-0427(00)00393-9},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Van Loan - 2000 - The ubiquitous Kronecker product.pdf:pdf},
isbn = {0377-0427},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
number = {1-2},
pages = {85--100},
title = {{The ubiquitous Kronecker product}},
url = {http://www.sciencedirect.com/science/article/pii/S0377042700003939},
volume = {123},
year = {2000}
}
@article{Leskovec2008,
abstract = {How can we model networks with a mathematically tractable model that allows for rigorous analysis of network properties? Networks exhibit a long list of surprising properties: heavy tails for the degree distribution; small diameters; and densification and shrinking diameters over time. Most present network models either fail to match several of the above properties, are complicated to analyze mathematically, or both. In this paper we propose a generative model for networks that is both mathematically tractable and can generate networks that have the above mentioned properties. Our main idea is to use the Kronecker product to generate graphs that we refer to as "Kronecker graphs". First, we prove that Kronecker graphs naturally obey common network properties. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks. We then present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super- exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques. Experiments on large real and synthetic networks show that KronFit finds accurate parameters that indeed very well mimic the properties of target networks. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null- models, anonymization, extrapolations, and graph summarization.},
archivePrefix = {arXiv},
arxivId = {0812.4905},
author = {Leskovec, Jure and Chakrabarti, Deepayan and Kleinberg, Jon and Faloutsos, Christos and Ghahramani, Zoubin},
eprint = {0812.4905},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Leskovec et al. - 2008 - Kronecker graphs an approach to modeling networks.pdf:pdf},
isbn = {1532-4435},
issn = {0012365X},
journal = {Journal of Machine Learning Research},
keywords = {Kronecker graphs,graph genera-tors,graph mining,network analysis,network evolution,network models,social networks},
pages = {985--1042},
pmid = {17746742},
title = {{Kronecker graphs: an approach to modeling networks}},
url = {https://cs.stanford.edu/{~}jure/pubs/kronecker-jmlr10.pdf http://arxiv.org/abs/0812.4905},
volume = {11},
year = {2008}
}
